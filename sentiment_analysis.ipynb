{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project: Sentiment Analysis (Fall 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Members**\n",
    "\n",
    "* Amit Sarvate (100794129)\n",
    "* Nirujan Velvarathan (100706828)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "* We aim to classify movie reviews into positive or negative sentiments using a large, popular dataset containing 50,000 instances. \n",
    "* To achieve this, we will experiment with three different network architectures: \n",
    "    * a Feedforward Neural Network with pre-trained embeddings, \n",
    "    * a Convolutional Neural Network (CNN), \n",
    "    * and a Gated Recurrent Unit (GRU). \n",
    "* The goal is to compare their performance on sentiment classification and identify the most effective model. \n",
    "* Additionally, we will develop an application where users can input a review and receive a sentiment prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing External Libraries\n",
    "\n",
    "In order to preprocess data as well as build, train and test our models - we will require various different essential ML libraries including pandas, sklearn, torch, and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feed-forward Neural Network (FNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiofiles==22.1.0 (from -r requirements.lock (line 1))\n",
      "  Using cached aiofiles-22.1.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting aiohttp==3.8.6 (from -r requirements.lock (line 2))\n",
      "  Using cached aiohttp-3.8.6-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.lock (line 3))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting aiosqlite==0.19.0 (from -r requirements.lock (line 4))\n",
      "  Using cached aiosqlite-0.19.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting alembic==1.9.1 (from -r requirements.lock (line 5))\n",
      "  Using cached alembic-1.9.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting annotated-types==0.6.0 (from -r requirements.lock (line 6))\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting anyio==3.6.2 (from -r requirements.lock (line 7))\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting appnope==0.1.4 (from -r requirements.lock (line 8))\n",
      "  Using cached appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
      "Collecting argon2-cffi==21.3.0 (from -r requirements.lock (line 9))\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting argon2-cffi-bindings==21.2.0 (from -r requirements.lock (line 10))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting arrow==1.3.0 (from -r requirements.lock (line 11))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting asttokens==2.2.1 (from -r requirements.lock (line 12))\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting async-generator==1.10 (from -r requirements.lock (line 13))\n",
      "  Using cached async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting async-timeout==4.0.3 (from -r requirements.lock (line 14))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting attrs==22.2.0 (from -r requirements.lock (line 15))\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Babel==2.11.0 (from -r requirements.lock (line 16))\n",
      "  Using cached Babel-2.11.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting backcall==0.2.0 (from -r requirements.lock (line 17))\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting backports.functools-lru-cache==1.6.4 (from -r requirements.lock (line 18))\n",
      "  Using cached backports.functools_lru_cache-1.6.4-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting beautifulsoup4==4.11.1 (from -r requirements.lock (line 19))\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting bleach==6.0.0 (from -r requirements.lock (line 20))\n",
      "  Using cached bleach-6.0.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting blinker==1.5 (from -r requirements.lock (line 21))\n",
      "  Using cached blinker-1.5-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting brotlipy==0.7.0 (from -r requirements.lock (line 22))\n",
      "  Using cached brotlipy-0.7.0-cp35-abi3-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting certifi==2022.12.7 (from -r requirements.lock (line 23))\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting certipy==0.1.3 (from -r requirements.lock (line 24))\n",
      "  Using cached certipy-0.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cffi==1.15.1 (from -r requirements.lock (line 25))\n",
      "  Using cached cffi-1.15.1-cp310-cp310-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting charset-normalizer==2.1.1 (from -r requirements.lock (line 26))\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\nirujan\\documents\\github\\sentiment-analysis-ml\\mymlenv\\lib\\site-packages (from -r requirements.lock (line 27)) (0.4.6)\n",
      "Collecting comm==0.2.0 (from -r requirements.lock (line 28))\n",
      "  Using cached comm-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting contourpy==1.2.0 (from -r requirements.lock (line 29))\n",
      "  Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cryptography==39.0.0 (from -r requirements.lock (line 30))\n",
      "  Using cached cryptography-39.0.0-cp36-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler==0.12.1 (from -r requirements.lock (line 31))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting debugpy==1.6.6 (from -r requirements.lock (line 32))\n",
      "  Using cached debugpy-1.6.6-cp310-cp310-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\nirujan\\documents\\github\\sentiment-analysis-ml\\mymlenv\\lib\\site-packages (from -r requirements.lock (line 33)) (5.1.1)\n",
      "Collecting defusedxml==0.7.1 (from -r requirements.lock (line 34))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting docopt==0.6.2 (from -r requirements.lock (line 35))\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji==2.8.0 (from -r requirements.lock (line 36))\n",
      "  Using cached emoji-2.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting entrypoints==0.4 (from -r requirements.lock (line 37))\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting executing==1.2.0 (from -r requirements.lock (line 38))\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting fastjsonschema==2.16.2 (from -r requirements.lock (line 39))\n",
      "  Using cached fastjsonschema-2.16.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting filelock==3.13.1 (from -r requirements.lock (line 40))\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting flit_core==3.8.0 (from -r requirements.lock (line 41))\n",
      "  Using cached flit_core-3.8.0-py3-none-any.whl.metadata (401 bytes)\n",
      "Collecting fonttools==4.44.3 (from -r requirements.lock (line 42))\n",
      "  Using cached fonttools-4.44.3-cp310-cp310-win_amd64.whl.metadata (157 kB)\n",
      "Collecting fqdn==1.5.1 (from -r requirements.lock (line 43))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting frozenlist==1.4.0 (from -r requirements.lock (line 44))\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting fsspec==2023.10.0 (from -r requirements.lock (line 45))\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gitdb==4.0.10 (from -r requirements.lock (line 46))\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting GitPython==3.1.37 (from -r requirements.lock (line 47))\n",
      "  Using cached GitPython-3.1.37-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting greenlet==2.0.1 (from -r requirements.lock (line 48))\n",
      "  Using cached greenlet-2.0.1-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting idna==3.4 (from -r requirements.lock (line 49))\n",
      "  Using cached idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting importlib-metadata==6.0.0 (from -r requirements.lock (line 50))\n",
      "  Using cached importlib_metadata-6.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting importlib-resources==5.10.2 (from -r requirements.lock (line 51))\n",
      "  Using cached importlib_resources-5.10.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting ipykernel==6.20.2 (from -r requirements.lock (line 52))\n",
      "  Using cached ipykernel-6.20.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting ipython==8.12.3 (from -r requirements.lock (line 53))\n",
      "  Using cached ipython-8.12.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ipython-genutils==0.2.0 (from -r requirements.lock (line 54))\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting ipywidgets==8.1.1 (from -r requirements.lock (line 55))\n",
      "  Using cached ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting isoduration==20.11.0 (from -r requirements.lock (line 56))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jedi==0.18.2 (from -r requirements.lock (line 57))\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting Jinja2==3.1.2 (from -r requirements.lock (line 58))\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting joblib==1.3.2 (from -r requirements.lock (line 59))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting json5==0.9.5 (from -r requirements.lock (line 60))\n",
      "  Using cached json5-0.9.5-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting jsonpointer==2.4 (from -r requirements.lock (line 61))\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting jsonschema==4.17.3 (from -r requirements.lock (line 62))\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting jupyter_client==7.4.9 (from -r requirements.lock (line 63))\n",
      "  Using cached jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter_core==5.1.4 (from -r requirements.lock (line 64))\n",
      "  Using cached jupyter_core-5.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jupyter-events==0.6.3 (from -r requirements.lock (line 65))\n",
      "  Using cached jupyter_events-0.6.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jupyter_server==2.1.0 (from -r requirements.lock (line 66))\n",
      "  Using cached jupyter_server-2.1.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting jupyter_server_fileid==0.9.0 (from -r requirements.lock (line 67))\n",
      "  Using cached jupyter_server_fileid-0.9.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting jupyter_server_proxy==4.1.0 (from -r requirements.lock (line 68))\n",
      "  Using cached jupyter_server_proxy-4.1.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter_server_terminals==0.4.4 (from -r requirements.lock (line 69))\n",
      "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jupyter_server_ydoc==0.8.0 (from -r requirements.lock (line 70))\n",
      "  Using cached jupyter_server_ydoc-0.8.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jupyter-telemetry==0.1.0 (from -r requirements.lock (line 71))\n",
      "  Using cached jupyter_telemetry-0.1.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jupyter-ydoc==0.2.5 (from -r requirements.lock (line 72))\n",
      "  Using cached jupyter_ydoc-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jupyterhub==3.1.1 (from -r requirements.lock (line 73))\n",
      "  Using cached jupyterhub-3.1.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jupyterlab==3.6.3 (from -r requirements.lock (line 74))\n",
      "  Using cached jupyterlab-3.6.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jupyterlab-pygments==0.2.2 (from -r requirements.lock (line 75))\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jupyterlab_server==2.19.0 (from -r requirements.lock (line 76))\n",
      "  Using cached jupyterlab_server-2.19.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting jupyterlab-widgets==3.0.9 (from -r requirements.lock (line 77))\n",
      "  Using cached jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting kaggle==1.5.16 (from -r requirements.lock (line 78))\n",
      "  Using cached kaggle-1.5.16.tar.gz (83 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kiwisolver==1.4.5 (from -r requirements.lock (line 79))\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting lightning==2.1.1 (from -r requirements.lock (line 80))\n",
      "  Using cached lightning-2.1.1-py3-none-any.whl.metadata (61 kB)\n",
      "Collecting lightning-utilities==0.9.0 (from -r requirements.lock (line 81))\n",
      "  Using cached lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting Mako==1.2.4 (from -r requirements.lock (line 82))\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.lock (line 83))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe==2.1.2 (from -r requirements.lock (line 84))\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting matplotlib==3.8.2 (from -r requirements.lock (line 85))\n",
      "  Using cached matplotlib-3.8.2-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting matplotlib-inline==0.1.6 (from -r requirements.lock (line 86))\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.lock (line 87))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mistune==2.0.4 (from -r requirements.lock (line 88))\n",
      "  Using cached mistune-2.0.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mpmath==1.3.0 (from -r requirements.lock (line 89))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting multidict==6.0.4 (from -r requirements.lock (line 90))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting nbclassic==0.4.8 (from -r requirements.lock (line 91))\n",
      "  Using cached nbclassic-0.4.8-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nbclient==0.7.2 (from -r requirements.lock (line 92))\n",
      "  Using cached nbclient-0.7.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting nbconvert==7.16.4 (from -r requirements.lock (line 93))\n",
      "  Using cached nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbformat==5.7.3 (from -r requirements.lock (line 94))\n",
      "  Using cached nbformat-5.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting nest-asyncio==1.5.6 (from -r requirements.lock (line 95))\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting networkx==3.2.1 (from -r requirements.lock (line 96))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting notebook==6.5.2 (from -r requirements.lock (line 97))\n",
      "  Using cached notebook-6.5.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting notebook_shim==0.2.2 (from -r requirements.lock (line 98))\n",
      "  Using cached notebook_shim-0.2.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numpy==1.26.0 (from -r requirements.lock (line 99))\n",
      "  Using cached numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting oauthlib==3.2.2 (from -r requirements.lock (line 100))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting packaging==23.0 (from -r requirements.lock (line 101))\n",
      "  Using cached packaging-23.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting pamela==1.0.0 (from -r requirements.lock (line 102))\n",
      "  Using cached pamela-1.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pandas==2.1.1 (from -r requirements.lock (line 103))\n",
      "  Using cached pandas-2.1.1-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Collecting pandocfilters==1.5.0 (from -r requirements.lock (line 104))\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parso==0.8.3 (from -r requirements.lock (line 105))\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pexpect==4.8.0 (from -r requirements.lock (line 106))\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pickleshare==0.7.5 (from -r requirements.lock (line 107))\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting Pillow==10.1.0 (from -r requirements.lock (line 108))\n",
      "  Using cached Pillow-10.1.0-cp310-cp310-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting pip==22.3.1 (from -r requirements.lock (line 109))\n",
      "  Using cached pip-22.3.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pipreqs==0.5.0 (from -r requirements.lock (line 110))\n",
      "  Using cached pipreqs-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pkgutil_resolve_name==1.3.10 (from -r requirements.lock (line 111))\n",
      "  Using cached pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
      "Collecting platformdirs==2.6.2 (from -r requirements.lock (line 112))\n",
      "  Using cached platformdirs-2.6.2-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pluggy==1.0.0 (from -r requirements.lock (line 113))\n",
      "  Using cached pluggy-1.0.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting portalocker==2.8.2 (from -r requirements.lock (line 114))\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting prometheus-client==0.16.0 (from -r requirements.lock (line 115))\n",
      "  Using cached prometheus_client-0.16.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting prompt-toolkit==3.0.36 (from -r requirements.lock (line 116))\n",
      "  Using cached prompt_toolkit-3.0.36-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting psutil==5.9.4 (from -r requirements.lock (line 117))\n",
      "  Using cached psutil-5.9.4-cp36-abi3-win_amd64.whl.metadata (21 kB)\n",
      "Collecting ptyprocess==0.7.0 (from -r requirements.lock (line 118))\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pure-eval==0.2.2 (from -r requirements.lock (line 119))\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pycparser==2.21 (from -r requirements.lock (line 120))\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pycurl==7.45.1 (from -r requirements.lock (line 121))\n",
      "  Using cached pycurl-7.45.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Nirujan\\AppData\\Local\\Temp\\pip-install-85q5te0f\\pycurl_2f32f368a6934acd8f652f075bd36a92\\setup.py\", line 954, in <module>\n",
      "          setup_args['cmdclass'] = {'bdist_msi': get_bdist_msi_version_hack()}\n",
      "        File \"C:\\Users\\Nirujan\\AppData\\Local\\Temp\\pip-install-85q5te0f\\pycurl_2f32f368a6934acd8f652f075bd36a92\\setup.py\", line 594, in get_bdist_msi_version_hack\n",
      "          from distutils.command.bdist_msi import bdist_msi\n",
      "      ModuleNotFoundError: No module named 'distutils.command.bdist_msi'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader # type: ignore\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import models \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING DATASET AND PREPROCESSING \n",
    "df_FNN = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_FNN = vectorizer.fit_transform(df_FNN['review']).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_FNN = label_encoder.fit_transform(df_FNN['sentiment'])\n",
    "\n",
    "X_train_FNN, X_test_FNN, y_train_FNN, y_test_FNN = train_test_split(X_FNN, y_FNN, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # self.data = data.clone().detach().float()\n",
    "        # self.labels = labels.clone().detach().long()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_FNN = SentimentDataset(X_train_FNN, y_train_FNN)\n",
    "dataset_te_FNN = SentimentDataset(X_test_FNN, y_test_FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_FNN = DataLoader(dataset_tr_FNN, batch_size=32, shuffle=True)\n",
    "loader_te_FNN = DataLoader(dataset_te_FNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3164\n",
      "Epoch [2/10], Loss: 0.2144\n",
      "Epoch [3/10], Loss: 0.1315\n",
      "Epoch [4/10], Loss: 0.0525\n",
      "Epoch [5/10], Loss: 0.0261\n",
      "Epoch [6/10], Loss: 0.0191\n",
      "Epoch [7/10], Loss: 0.0153\n",
      "Epoch [8/10], Loss: 0.0145\n",
      "Epoch [9/10], Loss: 0.0105\n",
      "Epoch [10/10], Loss: 0.0138\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_FNN.shape[1]\n",
    "hidden_dim = 500\n",
    "output_dim = 2 # positive and negative \n",
    "\n",
    "reload(models)\n",
    "model_FNN = models.FeedforwardNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model_FNN.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_FNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_FNN:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_FNN(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_FNN):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model_FNN, loader_te_FNN)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, review, vectorizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bow_vector = vectorizer.transform([review]).toarray()\n",
    "        bow_tensor = torch.tensor(bow_vector, dtype=torch.float32)\n",
    "        output = model(bow_tensor)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        return label_encoder.inverse_transform([prediction.item()])[0]\n",
    "\n",
    "new_review = \"The movie was not good! I hated it.\"\n",
    "print(\"Sentiment:\", predict_sentiment(model_FNN, new_review, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.1.0+cpu\n",
      "TorchText Version: 0.16.0+cpu\n",
      "Is CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"TorchText Version:\", torchtext.__version__)\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.read_csv(\"data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "max_features = 5000  \n",
    "max_len = 100  # Maximum sequence length\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Use basic English tokenizer\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Encode text as sequences of token indices\n",
    "def encode_text(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_indices = [vocab[token] for token in tokens]\n",
    "    return token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "def pad_sequence_to_max_len(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq += [0] * (max_len - len(seq))  # Padding with 0\n",
    "        else:\n",
    "            seq = seq[:max_len]  # Truncate if longer than max_len\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.tensor(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df_CNN['review']),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_features  # Limit vocab size to max_features\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary tokens\n",
    "\n",
    "# Encode the dataset\n",
    "X_CNN = [encode_text(review) for review in df_CNN['review']]\n",
    "\n",
    "X_CNN = pad_sequence_to_max_len(X_CNN, max_len)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\"positive\": 1, \"negative\": 0}  # Map sentiments to integers\n",
    "y_CNN = torch.tensor([label_mapping[label] for label in df_CNN['sentiment']])\n",
    "\n",
    "# Train-test split\n",
    "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN = train_test_split(X_CNN, y_CNN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_CNN = X_train_CNN.clone().detach()\n",
    "X_test_CNN = X_test_CNN.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.clone().detach().long()  \n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_CNN = SentimentDataset(X_train_CNN, y_train_CNN)\n",
    "dataset_te_CNN = SentimentDataset(X_test_CNN, y_test_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_CNN = DataLoader(dataset_tr_CNN, batch_size=32, shuffle=True)\n",
    "loader_te_CNN = DataLoader(dataset_te_CNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_features  \n",
    "embed_dim = 100            \n",
    "kernel_sizes = [3, 4, 5]   \n",
    "num_filters = 100          \n",
    "num_classes = 2\n",
    "\n",
    "reload(models)\n",
    "model_CNN = models.ConvolutionalNeuralNetwork(vocab_size, embed_dim, num_classes, kernel_sizes, num_filters)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5213\n",
      "Epoch [2/10], Loss: 0.3568\n",
      "Epoch [3/10], Loss: 0.2373\n",
      "Epoch [4/10], Loss: 0.1236\n",
      "Epoch [5/10], Loss: 0.0518\n",
      "Epoch [6/10], Loss: 0.0258\n",
      "Epoch [7/10], Loss: 0.0234\n",
      "Epoch [8/10], Loss: 0.0408\n",
      "Epoch [9/10], Loss: 0.0248\n",
      "Epoch [10/10], Loss: 0.0163\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_CNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_CNN:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_CNN(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_CNN):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m new_review2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe movie was not good! I hated it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpredict_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_review2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mpredict_sentiment\u001b[1;34m(model, review, vectorizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m bow_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([review])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      5\u001b[0m bow_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(bow_vector, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m _, prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform([prediction\u001b[38;5;241m.\u001b[39mitem()])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\models.py:15\u001b[0m, in \u001b[0;36mConvolutionalNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m     conv_outs \u001b[38;5;241m=\u001b[39m [F\u001b[38;5;241m.\u001b[39mrelu(conv(x))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs]\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gated Recurrent Network (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'c:\\\\Users\\\\Nirujan\\\\Documents\\\\GitHub\\\\sentiment-analysis-ml\\\\models.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        predictions, targets = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(targets, predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_predictions, val_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(classification_report(val_targets, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRUNeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      5\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Binary classification for sentiment analysis\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGRUNeuralNetwork\u001b[49m(vocab_size, embed_dim, hidden_dim, num_classes)\n\u001b[0;32m      8\u001b[0m train_model(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GRUNeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 10000  # Replace with your dataset's vocabulary size\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "num_classes = 2  # Binary classification for sentiment analysis\n",
    "\n",
    "model = GRUNeuralNetwork(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
