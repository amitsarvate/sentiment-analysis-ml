{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project: Sentiment Analysis (Fall 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Members**\n",
    "\n",
    "* Amit Sarvate (100794129)\n",
    "* Nirujan Velvarathan (100706828)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "* We aim to classify movie reviews into positive or negative sentiments using a large, popular dataset containing 50,000 instances. \n",
    "* To achieve this, we will experiment with three different network architectures: \n",
    "    * a Feedforward Neural Network with pre-trained embeddings, \n",
    "    * a Convolutional Neural Network (CNN), \n",
    "    * and a Gated Recurrent Unit (GRU). \n",
    "* The goal is to compare their performance on sentiment classification and identify the most effective model. \n",
    "* Additionally, we will develop an application where users can input a review and receive a sentiment prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing External Libraries\n",
    "\n",
    "In order to preprocess data as well as build, train and test our models - we will require various different essential ML libraries including pandas, sklearn, torch, and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feed-forward Neural Network (FNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader # type: ignore\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import models \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING DATASET AND PREPROCESSING \n",
    "df_FNN = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_FNN = vectorizer.fit_transform(df_FNN['review']).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_FNN = label_encoder.fit_transform(df_FNN['sentiment'])\n",
    "\n",
    "X_train_FNN, X_test_FNN, y_train_FNN, y_test_FNN = train_test_split(X_FNN, y_FNN, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # self.data = data.clone().detach().float()\n",
    "        # self.labels = labels.clone().detach().long()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_FNN = SentimentDataset(X_train_FNN, y_train_FNN)\n",
    "dataset_te_FNN = SentimentDataset(X_test_FNN, y_test_FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_FNN = DataLoader(dataset_tr_FNN, batch_size=32, shuffle=True)\n",
    "loader_te_FNN = DataLoader(dataset_te_FNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3164\n",
      "Epoch [2/10], Loss: 0.2144\n",
      "Epoch [3/10], Loss: 0.1315\n",
      "Epoch [4/10], Loss: 0.0525\n",
      "Epoch [5/10], Loss: 0.0261\n",
      "Epoch [6/10], Loss: 0.0191\n",
      "Epoch [7/10], Loss: 0.0153\n",
      "Epoch [8/10], Loss: 0.0145\n",
      "Epoch [9/10], Loss: 0.0105\n",
      "Epoch [10/10], Loss: 0.0138\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_FNN.shape[1]\n",
    "hidden_dim = 500\n",
    "output_dim = 2 # positive and negative \n",
    "\n",
    "reload(models)\n",
    "model_FNN = models.FeedforwardNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model_FNN.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_FNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_FNN:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_FNN(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_FNN):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model_FNN, loader_te_FNN)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, review, vectorizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bow_vector = vectorizer.transform([review]).toarray()\n",
    "        bow_tensor = torch.tensor(bow_vector, dtype=torch.float32)\n",
    "        output = model(bow_tensor)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        return label_encoder.inverse_transform([prediction.item()])[0]\n",
    "\n",
    "new_review = \"The movie was not good! I hated it.\"\n",
    "print(\"Sentiment:\", predict_sentiment(model_FNN, new_review, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.1.0+cpu\n",
      "TorchText Version: 0.16.0+cpu\n",
      "Is CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"TorchText Version:\", torchtext.__version__)\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.read_csv(\"data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "max_features = 5000  \n",
    "max_len = 100  # Maximum sequence length\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Use basic English tokenizer\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Encode text as sequences of token indices\n",
    "def encode_text(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_indices = [vocab[token] for token in tokens]\n",
    "    return token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "def pad_sequence_to_max_len(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq += [0] * (max_len - len(seq))  # Padding with 0\n",
    "        else:\n",
    "            seq = seq[:max_len]  # Truncate if longer than max_len\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.tensor(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df_CNN['review']),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_features  # Limit vocab size to max_features\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary tokens\n",
    "\n",
    "# Encode the dataset\n",
    "X_CNN = [encode_text(review) for review in df_CNN['review']]\n",
    "\n",
    "X_CNN = pad_sequence_to_max_len(X_CNN, max_len)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\"positive\": 1, \"negative\": 0}  # Map sentiments to integers\n",
    "y_CNN = torch.tensor([label_mapping[label] for label in df_CNN['sentiment']])\n",
    "\n",
    "# Train-test split\n",
    "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN = train_test_split(X_CNN, y_CNN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_CNN = X_train_CNN.clone().detach()\n",
    "X_test_CNN = X_test_CNN.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.clone().detach().long()  \n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_CNN = SentimentDataset(X_train_CNN, y_train_CNN)\n",
    "dataset_te_CNN = SentimentDataset(X_test_CNN, y_test_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_CNN = DataLoader(dataset_tr_CNN, batch_size=32, shuffle=True)\n",
    "loader_te_CNN = DataLoader(dataset_te_CNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_features  \n",
    "embed_dim = 100            \n",
    "kernel_sizes = [3, 4, 5]   \n",
    "num_filters = 100          \n",
    "num_classes = 2\n",
    "\n",
    "reload(models)\n",
    "model_CNN = models.ConvolutionalNeuralNetwork(vocab_size, embed_dim, num_classes, kernel_sizes, num_filters)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5213\n",
      "Epoch [2/10], Loss: 0.3568\n",
      "Epoch [3/10], Loss: 0.2373\n",
      "Epoch [4/10], Loss: 0.1236\n",
      "Epoch [5/10], Loss: 0.0518\n",
      "Epoch [6/10], Loss: 0.0258\n",
      "Epoch [7/10], Loss: 0.0234\n",
      "Epoch [8/10], Loss: 0.0408\n",
      "Epoch [9/10], Loss: 0.0248\n",
      "Epoch [10/10], Loss: 0.0163\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_CNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_CNN:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_CNN(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_CNN):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gated Recurrent Network (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'c:\\\\Users\\\\Nirujan\\\\Documents\\\\GitHub\\\\sentiment-analysis-ml\\\\models.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GRU = pd.read_csv(\"data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        predictions, targets = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(targets, predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_predictions, val_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_targets, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(classification_report(val_targets, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDatasetGRU(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.long()  # Ensure inputs are LongTensor\n",
    "        self.labels = labels.long()  # Ensure labels are LongTensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_GRU = [encode_text(review) for review in df_GRU['review']]\n",
    "X_GRU = pad_sequence_to_max_len(X_GRU, max_len).long()\n",
    "\n",
    "labels = torch.tensor([label_mapping[label] for label in df_GRU['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_GRU, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset_GRU = SentimentDatasetGRU(X_train, y_train)\n",
    "val_dataset_GRU = SentimentDatasetGRU(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "loader_tr_GRU  = DataLoader(train_dataset_GRU, batch_size=32, shuffle=True)\n",
    "loader_te_GRU  = DataLoader(val_dataset_GRU, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nirujan\\Documents\\GitHub\\sentiment-analysis-ml\\new_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6436667266368866, Accuracy: 0.6068\n",
      "Validation Accuracy: 0.7689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77      4961\n",
      "           1       0.77      0.76      0.77      5039\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "Epoch 2/10, Loss: 0.41464263858795164, Accuracy: 0.8104\n",
      "Validation Accuracy: 0.8237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      4961\n",
      "           1       0.81      0.84      0.83      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Epoch 3/10, Loss: 0.3286949941933155, Accuracy: 0.8571\n",
      "Validation Accuracy: 0.8270\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82      4961\n",
      "           1       0.81      0.85      0.83      5039\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Epoch 4/10, Loss: 0.26833968931734564, Accuracy: 0.8891\n",
      "Validation Accuracy: 0.8306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82      4961\n",
      "           1       0.81      0.87      0.84      5039\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Epoch 5/10, Loss: 0.20680388991385698, Accuracy: 0.9185\n",
      "Validation Accuracy: 0.8311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      4961\n",
      "           1       0.84      0.82      0.83      5039\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Epoch 6/10, Loss: 0.1433085219822824, Accuracy: 0.9460\n",
      "Validation Accuracy: 0.8254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83      4961\n",
      "           1       0.84      0.81      0.82      5039\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Epoch 7/10, Loss: 0.09543796399310231, Accuracy: 0.9672\n",
      "Validation Accuracy: 0.8229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      4961\n",
      "           1       0.83      0.81      0.82      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Epoch 8/10, Loss: 0.06914684803523123, Accuracy: 0.9767\n",
      "Validation Accuracy: 0.8213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      4961\n",
      "           1       0.83      0.82      0.82      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Epoch 9/10, Loss: 0.05263147195316851, Accuracy: 0.9828\n",
      "Validation Accuracy: 0.8168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.78      0.81      4961\n",
      "           1       0.80      0.85      0.82      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Epoch 10/10, Loss: 0.03937343142472673, Accuracy: 0.9872\n",
      "Validation Accuracy: 0.8201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81      4961\n",
      "           1       0.81      0.85      0.83      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 10000  # Replace with your dataset's vocabulary size\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "num_classes = 2  # Binary classification for sentiment analysis\n",
    "\n",
    "model_GRU = models.GRUNeuralNetwork(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "train_model(model_GRU, loader_tr_GRU, loader_te_GRU, epochs=10, learning_rate=0.001, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
