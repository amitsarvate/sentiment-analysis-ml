{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project: Sentiment Analysis (Fall 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Members**\n",
    "\n",
    "* Amit Sarvate (100794129)\n",
    "* Nirujan Velvarathan (100706828)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "* We aim to classify movie reviews into positive or negative sentiments using a large, popular dataset containing 50,000 instances. \n",
    "* To achieve this, we will experiment with three different network architectures: \n",
    "    * a Feedforward Neural Network with pre-trained embeddings, \n",
    "    * a Convolutional Neural Network (CNN), \n",
    "    * and a Gated Recurrent Unit (GRU). \n",
    "* The goal is to compare their performance on sentiment classification and identify the most effective model. \n",
    "* Additionally, we will develop an application where users can input a review and receive a sentiment prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing External Libraries\n",
    "\n",
    "In order to preprocess data as well as build, train and test our models - we will require various different essential ML libraries including pandas, sklearn, torch, and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feed-forward Neural Network (FNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import models \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING DATASET AND PREPROCESSING \n",
    "df_FNN = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_FNN = vectorizer.fit_transform(df_FNN['review']).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_FNN = label_encoder.fit_transform(df_FNN['sentiment'])\n",
    "\n",
    "X_train_FNN, X_test_FNN, y_train_FNN, y_test_FNN = train_test_split(X_FNN, y_FNN, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # self.data = data.clone().detach().float()\n",
    "        # self.labels = labels.clone().detach().long()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_FNN = SentimentDataset(X_train_FNN, y_train_FNN)\n",
    "dataset_te_FNN = SentimentDataset(X_test_FNN, y_test_FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_FNN = DataLoader(dataset_tr_FNN, batch_size=32, shuffle=True)\n",
    "loader_te_FNN = DataLoader(dataset_te_FNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3166\n",
      "Epoch [2/10], Loss: 0.2154\n",
      "Epoch [3/10], Loss: 0.1391\n",
      "Epoch [4/10], Loss: 0.0629\n",
      "Epoch [5/10], Loss: 0.0248\n",
      "Epoch [6/10], Loss: 0.0188\n",
      "Epoch [7/10], Loss: 0.0146\n",
      "Epoch [8/10], Loss: 0.0122\n",
      "Epoch [9/10], Loss: 0.0140\n",
      "Epoch [10/10], Loss: 0.0110\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_FNN.shape[1]\n",
    "hidden_dim = 500\n",
    "output_dim = 2 # positive and negative \n",
    "\n",
    "reload(models)\n",
    "model_FNN = models.FeedforwardNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model_FNN.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_FNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_FNN:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_FNN(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_FNN):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model_FNN, loader_te_FNN)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, review, vectorizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bow_vector = vectorizer.transform([review]).toarray()\n",
    "        bow_tensor = torch.tensor(bow_vector, dtype=torch.float32)\n",
    "        output = model(bow_tensor)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        return label_encoder.inverse_transform([prediction.item()])[0]\n",
    "\n",
    "new_review = \"The movie was not good! I hated it.\"\n",
    "print(\"Sentiment:\", predict_sentiment(model_FNN, new_review, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.read_csv(\"data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "max_features = 5000  \n",
    "max_len = 100  # Maximum sequence length\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # Use basic English tokenizer\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Encode text as sequences of token indices\n",
    "def encode_text(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_indices = [vocab[token] for token in tokens]\n",
    "    return token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "def pad_sequence_to_max_len(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq += [0] * (max_len - len(seq))  # Padding with 0\n",
    "        else:\n",
    "            seq = seq[:max_len]  # Truncate if longer than max_len\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.tensor(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df_CNN['review']),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_features  # Limit vocab size to max_features\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary tokens\n",
    "\n",
    "# Encode the dataset\n",
    "X_CNN = [encode_text(review) for review in df_CNN['review']]\n",
    "\n",
    "X_CNN = pad_sequence_to_max_len(X_CNN, max_len)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\"positive\": 1, \"negative\": 0}  # Map sentiments to integers\n",
    "y_CNN = torch.tensor([label_mapping[label] for label in df_CNN['sentiment']])\n",
    "\n",
    "# Train-test split\n",
    "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN = train_test_split(X_CNN, y_CNN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_CNN = X_train_CNN.clone().detach()\n",
    "X_test_CNN = X_test_CNN.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.clone().detach().long()  \n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr_CNN = SentimentDataset(X_train_CNN, y_train_CNN)\n",
    "dataset_te_CNN = SentimentDataset(X_test_CNN, y_test_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr_CNN = DataLoader(dataset_tr_CNN, batch_size=32, shuffle=True)\n",
    "loader_te_CNN = DataLoader(dataset_te_CNN, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_features  \n",
    "embed_dim = 100            \n",
    "kernel_sizes = [3, 4, 5]   \n",
    "num_filters = 100          \n",
    "num_classes = 2\n",
    "\n",
    "reload(models)\n",
    "model_CNN = models.ConvolutionalNeuralNetwork(vocab_size, embed_dim, num_classes, kernel_sizes, num_filters)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5155\n",
      "Epoch [2/10], Loss: 0.3597\n",
      "Epoch [3/10], Loss: 0.2380\n",
      "Epoch [4/10], Loss: 0.1234\n",
      "Epoch [5/10], Loss: 0.0574\n",
      "Epoch [6/10], Loss: 0.0267\n",
      "Epoch [7/10], Loss: 0.0204\n",
      "Epoch [8/10], Loss: 0.0454\n",
      "Epoch [9/10], Loss: 0.0266\n",
      "Epoch [10/10], Loss: 0.0204\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_CNN.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in loader_tr_CNN:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_CNN(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(loader_tr_CNN):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
